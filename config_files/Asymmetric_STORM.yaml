Task: "JointTrainAgent"

BasicSettings:
  Seed: 0
  ImageSize: 224
  ReplayBufferOnGPU: True
  dtype: torch.float16
  pretrained: None
  n: Boxing-life_done-wm_2L512D8H-100k-seed1
  config_path: config_files/Asymmetric_STORM.yam
  env_name: ALE/Boxing-v5
  trajectory_path: trajectories/Boxing/Boxing.pkl
  device: cuda:1
  pretrained_path: None


hydra:
  run:
    dir: /space/cristianmeo/STORM #/projects/0/prjs0951/STORM


JointTrainAgent:
  SampleMaxSteps: 163000
  BufferMaxLength: 100000
  BufferWarmUp: 1024
  NumEnvs: 1
  BatchSize: 4
  DemonstrationBatchSize: 4
  BatchLength: 16
  ImagineBatchSize: 16
  ImagineDemonstrationBatchSize: 256
  ImagineContextLength: 16
  ImagineBatchLength: 16
  TrainDynamicsEverySteps: 1
  TrainAgentEverySteps: 1
  UseDemonstration: False
  SaveEverySteps: 1030

Models:
  WorldModel:
    InChannels: 3
    TransformerMaxLength: 341 # (sequence length + 1)
    TransformerHiddenDim: 256
    TransformerNumLayers: 4
    TransformerNumHeads: 8
    Transformer: TransformerXL
    tokens_per_block: 8  #17
    max_blocks: 20
    attention: 'causal'
    num_layers: 10
    num_heads: 4
    embed_dim: 256
    embed_pdrop: 0.1
    resid_pdrop: 0.1
    attn_pdrop: 0.1
    model: 'Asymmetric-OC-STORM'
    continuos_embed_dim: 128
    dyn_num_heads: 4
    dyn_num_layers: 4
    dyn_feedforward_dim: 1024
    dyn_head_dim: 64
    dyn_z_dims: [512, 512, 512, 512]
    dyn_reward_dims: [256, 256, 256, 256]
    dyn_discount_dims: [256, 256, 256, 256]
    dyn_input_rewards: True
    dyn_input_discounts: False
    dyn_act: 'silu'
    dyn_norm: 'none'
    dyn_dropout: 0.1
    dyn_lr: 1e-4
    dyn_wd: 1e-6
    dyn_eps: 1e-5
    dyn_grad_clip: 100
    dyn_z_coef: 1
    dyn_reward_coef: 10
    dyn_discount_coef: 50
    wm_batch_size: 100
    wm_sequence_length: 340
    wm_train_steps: 1
    wm_memory_length: 21 # keep at 3 * # slots
    wm_discount_threshold: 0.1
    regularization_post_quant: False
    regularization_tokens: False
    regularization_embeddings: False
    embedding_input: False
    slot_based: True
    slot_regularization: False
    regularization_k_pred: False
    vit_model_name: samvit_base_patch16
    vit_use_pretrained: True
    vit_freeze: True
    vit_feature_level: 12
    independent_modules: True
    stochastic_head: False
    stochastic_dim: 32  #16*16 = 256
    action_emb_dim: 128
    wm_oc_pool_layer: dino-mlp 
    mixer_type: z+attn


    transformer_layer:
      embed_dim: 256
      feedforward_dim: 512
      head_dim: 64
      num_heads: 4
      activation: 'silu' 
      dropout_p: 0.1
      layer_norm_eps: 1e-5

  Decoder:
    resolution: 64
    dec_input_dim: 96
    dec_hidden_dim: 64
    out_ch: 4
    vit_num_patches: 196 # res 224
    dec_hidden_layers: [1024, 1024, 1024] # MLPDecoder
    dec_output_dim: 768 # MLPDecoder

  Slot_attn:
    num_slots: 9 # num_tokens = numx_slots * tokens_per_slot
    tokens_per_slot: 1
    iters: 3
    channels_enc: 96
    token_dim: 96 # need to match embed_dim if no pre_process_conv
    prior_class: grucell
    pred_prior_from: last

  Agent:
    NumLayers: 3
    HiddenDim: 512
    Gamma: 0.985
    Lambda: 0.95
    EntropyCoef: 3E-4
    pooling_layer: dino-mlp  #cls-transformer dino-sbd
    state: joint

  CLSTransformer:
    HiddenDim: 256
    NumLayers: 2
    NumHeads: 8
    z_dim: 96
 

