Task: "JointTrainAgent"

BasicSettings:
  Seed: 0
  ImageSize: 224
  ReplayBufferOnGPU: True
  dtype: torch.float16

JointTrainAgent:
  SampleMaxSteps: 102000
  BufferMaxLength: 100000
  BufferWarmUp: 16
  NumEnvs: 1
  BatchSize: 2
  DemonstrationBatchSize: 2
  BatchLength: 2
  ImagineBatchSize: 16
  ImagineDemonstrationBatchSize: 2
  ImagineContextLength: 2
  ImagineBatchLength: 4
  TrainDynamicsEverySteps: 1
  TrainAgentEverySteps: 1
  UseDemonstration: False
  SaveEverySteps: 50

Models:
  WorldModel:
    InChannels: 3
    TransformerMaxLength: 341 # (sequence length + 1)
    TransformerHiddenDim: 512
    TransformerNumLayers: 2
    TransformerNumHeads: 8
    Transformer: TransformerXL
    tokens_per_block: 8  #17
    max_blocks: 20
    attention: 'causal'
    num_layers: 10
    num_heads: 4
    embed_dim: 256
    embed_pdrop: 0.1
    resid_pdrop: 0.1
    attn_pdrop: 0.1
    model: 'OC-irisXL'
    continuos_embed_dim: 128
    dyn_num_heads: 4
    dyn_num_layers: 4
    dyn_feedforward_dim: 1024
    dyn_head_dim: 64
    dyn_z_dims: [512, 512, 512, 512]
    dyn_reward_dims: [256, 256, 256, 256]
    dyn_discount_dims: [256, 256, 256, 256]
    dyn_input_rewards: True
    dyn_input_discounts: False
    dyn_act: 'silu'
    dyn_norm: 'none'
    dyn_dropout: 0.1
    dyn_lr: 1e-4
    dyn_wd: 1e-6
    dyn_eps: 1e-5
    dyn_grad_clip: 100
    dyn_z_coef: 1
    dyn_reward_coef: 10
    dyn_discount_coef: 50
    wm_batch_size: 100
    wm_sequence_length: 340
    wm_train_steps: 1
    wm_memory_length: 21
    wm_discount_threshold: 0.1
    regularization_post_quant: False
    regularization_tokens: False
    regularization_embeddings: False
    embedding_input: False
    slot_based: True
    slot_regularization: False
    regularization_k_pred: False
    vit_model_name: samvit_base_patch16
    vit_use_pretrained: True
    vit_freeze: True
    vit_feature_level: 12
    use_onehot: True

    transformer_layer:
      embed_dim: 512
      feedforward_dim: 1024
      head_dim: 64
      num_heads: 4
      activation: 'silu' 
      dropout_p: 0.1
      layer_norm_eps: 1e-5

  Decoder:
    resolution: 224
    dec_input_dim: 128
    dec_hidden_dim: 64
    out_ch: 4
    vit_num_patches: 196 # res 224
    dec_hidden_layers: [1024, 1024, 1024] # MLPDecoder
    dec_output_dim: 768 # MLPDecoder

  Slot_attn:
    num_slots: 7 # num_tokens = num_slots * tokens_per_slot
    tokens_per_slot: 1
    iters: 3
    channels_enc: 128
    token_dim: 128 # need to match embed_dim if no pre_process_conv
    prior_class: gru

  Agent:
    NumLayers: 2
    HiddenDim: 512
    Gamma: 0.985
    Lambda: 0.95
    EntropyCoef: 3E-4